{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "ls: cannot access /tuples: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "!ls /tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numexpr\n",
    "import numpy\n",
    "import root_numpy\n",
    "import pandas as pd\n",
    "from rep import utils\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from rep.report.metrics import RocAuc\n",
    "from rep.metaml import GridOptimalSearchCV, FoldingScorer, RandomParameterOptimizer, SubgridParameterOptimizer\n",
    "from rep.estimators import SklearnClassifier, TMVAClassifier, XGBoostRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from rep.metaml import RegressionParameterOptimizer\n",
    "from rep.metaml import RandomParameterOptimizer\n",
    "from sklearn.gaussian_process import GaussianProcess\n",
    "from rep.report.metrics import OptimalMetric, ams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# importing data\n",
    "sigbranches = ['B_s0_TAU_ps', 'mu_MINIPS', 'B_s0_IP_OWNPV', 'B_s0_doca', 'sum_isolation', 'B_s0_PT',\n",
    "               'B_s0_cosnk', 'B_s0_CDFiso', 'mu_MINPT', 'B_s0_IPCHI2_OWNPV', 'B_s0_FD_OWNPV', 'B_s0_BDTS']#, 'mu_DeltaEta', 'mu_AbsPhi', 'B_s0_TAUCHI2',\n",
    "              #'B_s0_IPS_OWNPV', 'B_s0_ACOSDIRA_OWNPV', 'B_s0_ENDVERTEX_CHI2', 'B_s0_IPCHI2_OWNPV', 'B_s0_FD_OWNPV']\n",
    "bkgbranchesSS = sigbranches#+['B_s0_SSweight']\n",
    "bkgbranchesOS = sigbranches\n",
    "\n",
    "\n",
    "signaltrain = root_numpy.root2array(\"/tuples/MC_emu_S21p1_2.root\", \n",
    "                             treename='DecayTree', \n",
    "                             branches=sigbranches)\n",
    "#                             selection=\"B_s0_BDTS>0.05\")\n",
    "signaltrain = pd.DataFrame.from_records(signaltrain)\n",
    "\n",
    "\n",
    "backgroundtrain = root_numpy.root2array(\"/tuples/BEmu_SS_S21p1_reweighted_2.root\", \n",
    "                             treename='DecayTree', \n",
    "                             branches=bkgbranchesSS)\n",
    "#                             selection=\"B_s0_BDTS>0.05\")\n",
    "backgroundtrain = pd.DataFrame.from_records(backgroundtrain)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "signaltest = root_numpy.root2array(\"/tuples/MC_emu_S21p1_1.root\", \n",
    "                             treename='DecayTree', \n",
    "                             branches=sigbranches)\n",
    "#                             selection=\"B_s0_BDTS>0.05\")\n",
    "signaltest = pd.DataFrame.from_records(signaltest)\n",
    "\n",
    "\n",
    "backgroundtest0 = root_numpy.root2array(\"/tuples/BEmu_OS_S21p1.root\", \n",
    "                             treename='DecayTree', \n",
    "                             branches=bkgbranchesOS)\n",
    "#                             selection=\"B_s0_BDTS>0.05\")\n",
    "backgroundtest = pd.DataFrame.from_records(backgroundtest0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_labels=concatenate((np.ones(signaltrain.shape[0]),\n",
    "                          np.zeros(backgroundtrain.shape[0])))\n",
    "test_labels=concatenate((np.ones(signaltest.shape[0]),\n",
    "                          np.zeros(backgroundtest.shape[0])))\n",
    "\n",
    "\n",
    "train_data = pd.concat([signaltrain, backgroundtrain], axis=0)\n",
    "test_data = pd.concat([signaltest, backgroundtest], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "re-implement this function since I cannot import it from sklearn.base\n",
    "\"\"\"\n",
    "# Authors: Alexandre Gramfort <alexandre.gramfort@inria.fr>\n",
    "#          Mathieu Blondel <mathieu@mblondel.org>\n",
    "#          Olivier Grisel <olivier.grisel@ensta.org>\n",
    "#          Arnaud Joly <a.joly@ulg.ac.be>\n",
    "#          Jochen Wersdorfer <jochen@wersdoerfer.de>\n",
    "#          Lars Buitinck\n",
    "#          Joel Nothman <joel.nothman@gmail.com>\n",
    "#          Noel Dawe <noel@dawe.me>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.utils import check_array, check_consistent_length\n",
    "from sklearn.utils.multiclass import type_of_target\n",
    "\n",
    "\n",
    "def average_binary_score(binary_metric, y_true, y_score, average,\n",
    "                          sample_weight=None):\n",
    "    \"\"\"Average a binary metric for multilabel classification\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array, shape = [n_samples] or [n_samples, n_classes]\n",
    "        True binary labels in binary label indicators.\n",
    "    y_score : array, shape = [n_samples] or [n_samples, n_classes]\n",
    "        Target scores, can either be probability estimates of the positive\n",
    "        class, confidence values, or binary decisions.\n",
    "    average : string, [None, 'micro', 'macro' (default), 'samples', 'weighted']\n",
    "        If ``None``, the scores for each class are returned. Otherwise,\n",
    "        this determines the type of averaging performed on the data:\n",
    "        ``'micro'``:\n",
    "            Calculate metrics globally by considering each element of the label\n",
    "            indicator matrix as a label.\n",
    "        ``'macro'``:\n",
    "            Calculate metrics for each label, and find their unweighted\n",
    "            mean.  This does not take label imbalance into account.\n",
    "        ``'weighted'``:\n",
    "            Calculate metrics for each label, and find their average, weighted\n",
    "            by support (the number of true instances for each label).\n",
    "        ``'samples'``:\n",
    "            Calculate metrics for each instance, and find their average.\n",
    "    sample_weight : array-like of shape = [n_samples], optional\n",
    "        Sample weights.\n",
    "    binary_metric : callable, returns shape [n_classes]\n",
    "        The binary metric function to use.\n",
    "    Returns\n",
    "    -------\n",
    "    score : float or array of shape [n_classes]\n",
    "        If not ``None``, average the score, else return the score for each\n",
    "        classes.\n",
    "    \"\"\"\n",
    "    average_options = (None, 'micro', 'macro', 'weighted', 'samples')\n",
    "    if average not in average_options:\n",
    "        raise ValueError('average has to be one of {0}'\n",
    "                         ''.format(average_options))\n",
    "\n",
    "    y_type = type_of_target(y_true)\n",
    "    if y_type not in (\"binary\", \"multilabel-indicator\"):\n",
    "        raise ValueError(\"{0} format is not supported\".format(y_type))\n",
    "\n",
    "    if y_type == \"binary\":\n",
    "        return binary_metric(y_true, y_score, sample_weight=sample_weight)\n",
    "\n",
    "    check_consistent_length(y_true, y_score, sample_weight)\n",
    "    y_true = check_array(y_true)\n",
    "    y_score = check_array(y_score)\n",
    "\n",
    "    not_average_axis = 1\n",
    "    score_weight = sample_weight\n",
    "    average_weight = None\n",
    "\n",
    "    if average == \"micro\":\n",
    "        if score_weight is not None:\n",
    "            score_weight = np.repeat(score_weight, y_true.shape[1])\n",
    "        y_true = y_true.ravel()\n",
    "        y_score = y_score.ravel()\n",
    "\n",
    "    elif average == 'weighted':\n",
    "        if score_weight is not None:\n",
    "            average_weight = np.sum(np.multiply(\n",
    "                y_true, np.reshape(score_weight, (-1, 1))), axis=0)\n",
    "        else:\n",
    "            average_weight = np.sum(y_true, axis=0)\n",
    "        if average_weight.sum() == 0:\n",
    "            return 0\n",
    "\n",
    "    elif average == 'samples':\n",
    "        # swap average_weight <-> score_weight\n",
    "        average_weight = score_weight\n",
    "        score_weight = None\n",
    "        not_average_axis = 0\n",
    "\n",
    "    if y_true.ndim == 1:\n",
    "        y_true = y_true.reshape((-1, 1))\n",
    "\n",
    "    if y_score.ndim == 1:\n",
    "        y_score = y_score.reshape((-1, 1))\n",
    "\n",
    "    n_classes = y_score.shape[not_average_axis]\n",
    "    score = np.zeros((n_classes,))\n",
    "    for c in range(n_classes):\n",
    "        y_true_c = y_true.take([c], axis=not_average_axis).ravel()\n",
    "        y_score_c = y_score.take([c], axis=not_average_axis).ravel()\n",
    "        score[c] = binary_metric(y_true_c, y_score_c,\n",
    "                                 sample_weight=score_weight)\n",
    "\n",
    "    # Average the results\n",
    "    if average is not None:\n",
    "        return np.average(score, weights=average_weight)\n",
    "    else:\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def roc_auc_score(y_true, y_score, average=\"macro\", sample_weight=None):\n",
    "    def _binary_roc_auc_score(y_true, y_score, sample_weight=None):\n",
    "        if len(np.unique(y_true)) != 2:\n",
    "            raise ValueError(\"Only one class present in y_true. ROC AUC score \"\n",
    "                             \"is not defined in that case.\")\n",
    "\n",
    "        fpr, tpr, tresholds = roc_curve(y_true, y_score,\n",
    "                                        sample_weight=sample_weight)\n",
    "    \n",
    "        fpr_highRej = numpy.array([]) #fpr is false positive rate\n",
    "        tpr_highRej = numpy.array([]) #tpr is true positive rate\n",
    "        for index, element in enumerate(fpr):  #selecting only stuff with rej > 0.9\n",
    "\n",
    "            if ((1 - element) > 0.9):\n",
    "                fpr_highRej = np.append(fpr_highRej, element)\n",
    "                tpr_highRej = np.append(tpr_highRej, tpr[index])\n",
    "\n",
    "        return auc(fpr_highRej, tpr_highRej, reorder=True)\n",
    "\n",
    "    return average_binary_score(\n",
    "        _binary_roc_auc_score, y_true, y_score, average,\n",
    "        sample_weight=sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from sklearn.metrics import roc_auc_score\n",
    "from sklearn.base import clone\n",
    "\n",
    "class MyScorer(object):\n",
    "    def __init__(self, test_data, test_labels):\n",
    "        self.test_data = test_data\n",
    "        self.test_labels = test_labels\n",
    "        \n",
    "    def __call__(self, base_estimator, params, X, y, sample_weight=None):\n",
    "        cl = clone(base_estimator)\n",
    "        cl.set_params(**params)\n",
    "        cl.fit(X, y)\n",
    "        pred = cl.predict_proba(self.test_data)\n",
    "        # Returning with minus, because we maximize metric\n",
    "        return roc_auc_score(self.test_labels, pred[:,1])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import randint, uniform\n",
    "now = datetime.datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "old_stdout = sys.stdout\n",
    "sys.stdout = open(\"training_stdout_{0}.txt\".format(now), 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# define grid parameters\n",
    "grid_param = {}\n",
    "grid_param['Ntrees'] = randint(2000, 15000)\n",
    "grid_param['MaxDepth'] = randint(2,5)\n",
    "grid_param['MinNodeSize'] = ['0.2%', '0.3%', '0.5%', '1%', '2%', '3%']\n",
    "grid_param['NCuts'] = randint(10,40)\n",
    "grid_param['AdaBoostBeta'] = uniform(0.1, 0.4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# using Gaussian Processes \n",
    "generator = RandomParameterOptimizer(grid_param, n_evaluations=24)\n",
    "\n",
    "estimator = TMVAClassifier(method='kBDT', features=sigbranches)\n",
    "scorer = MyScorer(test_data, test_labels)\n",
    "\n",
    "grid_finder = GridOptimalSearchCV(estimator, generator, scorer, parallel_profile='threads-8')\n",
    "grid_finder.fit(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.stdout = open('scores_{0}.txt'.format(now), 'w')\n",
    "grid_finder.params_generator.print_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(grid_finder.generator.grid_scores_.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "estim = grid_finder.fit_best_estimator(train_data, train_labels, sample_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "report = estim.test_on(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "figure(figsize(10, 8))\n",
    "report.roc(physics_notion=True).plot(xlim=(0, 1), ylim=(0, 1))\n",
    "plt.savefig(\"best_roc.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.stdout = old_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
